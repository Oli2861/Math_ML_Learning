{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Support Vector Machine (SVM)\n",
    "Using only numpy\n",
    "Based on [CS229](https://www.youtube.com/watch?v=lDwow4aOrtg) and [CS229](https://www.youtube.com/watch?v=8NYoQiRANpg), [ML Mastery](https://www.youtube.com/watch?v=bxGceB5Eq1c), [Visually Explained](https://www.youtube.com/watch?v=Q7vT0--5VII), [link](https://www.adeveloperdiary.com/data-science/machine-learning/support-vector-machines-for-beginners-duality-problem/).\n",
    "> A *support vector machine* can perform binary classification and regression tasks. It uses a hyperplane to separate the data into classes.\n",
    "> The *hyperplane* is chosen to maximize the margin between the classes (maximum margin separators).\n",
    "> *Support vectors* are the data points that are closest to the hyperplane and used to determine the hyperplane.\n",
    "\n",
    "## Notation\n",
    "- Hypothesis: $$h_{w,b}(x) = g(w^Tx + b)$$\n",
    "- Labels: $y \\in \\{-1, 1\\}$\n",
    "- $h$ output values in $\\{-1, 1\\}$\n",
    "- $g(z) = \\begin{cases} 1 & \\text{if } z \\geq 0 \\\\ -1 & otherwise \\end{cases}$ (logistic function)\n",
    "- $w$ is the weight vector without bias, as bias explicitly added as $b$\n",
    "- $w^Tx + b$ is the decision boundary\n",
    "- $g$ is the activation function (logistic function)\n",
    "- $\\gamma$ is the geometric margin\n",
    "- $\\hat{\\gamma}$ is the functional margin\n",
    "- $\\phi$ is the feature mapping function $\\phi: \\mathbb{R}^n \\rightarrow \\mathbb{R}^m$ (maps the data from the input space to a higher dimensional feature space)\n",
    "- $\\langle x,z \\rangle$ is the inner product of $x$ and $z$: $x^Tz$\n",
    "\n",
    "## Hyperplane\n",
    "- It is always of one dimension less than the data: 2D data -> 1D line, 3D data -> 2D plane, 4D data -> 3D plane, etc.\n",
    "- Functional margin of hyperplane defined by $(w, b)$ with respect to training example $(x^{(i)}, y^{(i)})$: $$\\hat{\\gamma}^{(i)} = y^{(i)}(w^Tx^{(i)} + b)$$\n",
    "- Desired properties:\n",
    "    - $y^{(i)} = 1$, want $w^Tx^{(i)} + b >> 1$ (want the result to be large and positive)\n",
    "    - $y^{(i)} = -1$, want $w^Tx^{(i)} + b << -1$ (want the result to be large and negative)\n",
    "- If $\\hat{\\gamma}^{(i)} > 0$, then $h(x^{(i)}) = y^{(i)}$ (as long as the functional margin is greater than 0, the prediction is correct)\n",
    "- Functional margin with respect to the training set: $\\hat{\\gamma} = \\min_{i=1,...,m} \\hat{j}^{(i)}$ (the smallest functional margin of any of the training examples -> how well the hyperplane separates the data over the entire training set)\n",
    "- Geometric margin of hyperplane defined by $(w, b)$ with respect to training example $(x^{(i)}, y^{(i)})$: $\\gamma^{(i)} = (\\frac{w^Tx^{(i)} + b}{\\vert \\vert w \\vert \\vert}) = \\frac{\\hat{\\gamma}^{(i)}}{\\vert \\vert w \\vert \\vert}$ (the functional margin divided by the Euclidean norm of w)\n",
    "- Geometric margin with respect to the training set: $\\gamma = \\min_{i=1,...,m} \\gamma^{(i)}$ (smallest geometric margin of any of the training examples)\n",
    "\n",
    "## Optimal margin classifier\n",
    "- Choose the parameters $w$ and $b$ to maximize $\\gamma$ (the geometric margin)\n",
    "- $max_{w, b, \\gamma} \\gamma$ subject to $\\hat{\\gamma}^{(i)} \\frac{w^Tx^{(i)} + b}{\\vert \\vert w \\vert \\vert} \\geq \\gamma$ for $i = 1, ..., m$ (the functional margin of any training example must be greater than or equal to the geometric margin)\n",
    "- Can be reformulated to a equivalent problem: $min_{w, b} \\vert \\vert w \\vert \\vert^2$ subject to $y^{(i)}(w^Tx^{(i)} + b) \\geq 1$ for $i = 1, ..., m$ (minimize the euclidean norm of w, subject to the functional margin of any training example being greater than or equal to 1)\n",
    "- Raising the geometric margin is the same as lowering the euclidean norm of w\n",
    "\n",
    "When the $min_i \\gamma^{(i)}$ shall be as large as possible, one possible way is to raise $\\gamma^{(i)} \\geq \\gamma$ for all $i$ (the geometric margin of any training example must be greater than or equal to the geometric margin of the entire training set) --> maximizing the geometric margin has the effect of maximizing the worst case geometric margin\n",
    "\n",
    "Scaling the length of the vectors by a constant factor does not change their direction.\n",
    "--> If $\\vert \\vert w \\vert \\vert$ is scaled such that it is equal to $\\frac{1}{\\gamma}$,\n",
    "the optimization problem becomes $max \\frac{1}{\\vert \\vert w \\vert \\vert}$ subject to $y^{(i)}(w^Tx^{(i)} + b) \\gamma \\geq \\gamma$ = $y^{(i)}(w^Tx^{(i)} + b) \\geq 1$ (maximize the length of w, subject to the functional margin of any training example being greater than or equal to 1)\n",
    "This is equal to $min \\frac{1}{2} \\vert \\vert w \\vert \\vert^2$ subject to $y^{(i)}(w^Tx^{(i)} + b) \\geq 1$ (minimize the euclidean norm of w, subject to the functional margin of any training example being greater than or equal to 1)\n",
    "\n",
    "## Support Vector Machine (SVM)\n",
    "- **Optimal margin classifier + kernel trick (non-linear classification) = support vector machine**\n",
    "    - The optimal margin classifier is a *quadratic programming problem* (can be solved efficiently)\n",
    "    - The *support vector machine* is a *soft margin classifier* by applying the *kernel trick* (non-linear classification)\n",
    "\n",
    "-  Represented theorem: Suppose $w = \\sum_{i=1}^m \\alpha_i x^{(i)}$ (w can be represented as a linear combination of the training examples)\n",
    "    - Why?\n",
    "        - Intuition 1: In logistic regression, the on every iteration the weights are updated by adding the product of the learning rate and the gradient of the cost function with respect to the weights. --> The weights are a linear combination of the training examples.\n",
    "        - Intuition 2: ([video](https://www.youtube.com/watch?v=8NYoQiRANpg&list=PLoROMvodv4rMiGQp3WXShtMGgzqpfVfbU&index=7) at 16:00)\n",
    "\n",
    "Then $min_{w, b} \\frac{1}{2} \\vert \\vert w \\vert \\vert^2$ subject to $y^{(i)}(w^Tx^{(i)} + b) \\geq 1$ for $i = 1, ..., m$ is equivalent to: $min_{\\alpha} \\frac{1}{2} \\sum_{i=1}^m \\sum_{j=1}^m \\alpha_i \\alpha_j y^{(i)} y^{(j)} x^{(i)T} x^{(j)} - \\sum_{i=1}^m \\alpha_i$ subject to $\\alpha_i \\geq 0$ for $i = 1, ..., m$ and $\\sum_{i=1}^m \\alpha_i y^{(i)} = 0$.\n",
    "\n",
    "Plug $w = \\sum_{i=1}^m \\alpha_i x^{(i)}$ into $min_{w, b} \\frac{1}{2} \\vert \\vert w \\vert \\vert^2$ subject to $y^{(i)}(w^Tx^{(i)} + b) \\geq 1$ for $i = 1, ..., m$: $min \\frac{1}{2}(\\sum_{i=1}^m \\alpha_iy^{(i)}x^{(i)})^T(\\sum_{j=1}^m \\alpha_jy^{(j)}x^{(j)})$ = $min \\frac{1}{2} \\sum_{i=1}^m \\sum_{j=1}^m \\alpha_i \\alpha_j y^{(i)} y^{(j)} x^{(i)T} x^{(j)}$\n",
    "The constraint becomes $y^{(i)}(\\sum_{j=1}^m \\alpha_jy^{(j)}x^{(j)})^Tx^{(i)} + b) \\geq 1$ for $i = 1, ..., m$.\n",
    "\n",
    "The optimization Problem can be simplified into the *Dual optimization problem*: $\\sum_i \\alpha_i - \\frac{1}{2} \\sum_i \\sum_j y^{(i)} y^{(j)} \\alpha_i \\alpha_j \\langle x^{(i)}, x^{(j)} \\rangle$ subject to $\\alpha_i \\geq 0$ for $i = 1, ..., m$ and $\\sum_{i=1}^m \\alpha_i y^{(i)} = 0$.\n",
    "--> Solve for $\\alpha_i$'s instead of $w$ and $b$.\n",
    "--> Make predictions using $w = \\sum_{i=1}^m \\alpha_i x^{(i)}$. --> $h_{w,b}(x) = g(w^Tx + b) = g((\\sum_{i=1}^m \\alpha_i x^{(i)})^Tx + b) = g(\\sum_{i=1}^m \\alpha_i \\langle x^{(i)}, x \\rangle + b)$\n",
    "- *Kernel trick*:\n",
    "    - Write algorithm in terms of $\\langle x^{(i)}, x^{(j)} \\rangle$ (or $\\langle x, z \\rangle$ instead of $x^{(i)}$ and $x^{(j)}$.\n",
    "    - Let there be some mapping from $x \\rightarrow \\phi(x)$.\n",
    "    - Find way to compute $K(x, z) = \\phi(x)^T \\phi(z)$ without explicitly computing $\\phi(x)$ and $\\phi(z)$.\n",
    "    - Replace $\\langle x, z \\rangle$ with $K(x, z)$.\n",
    "\n",
    "$K(x,z) = \\phi(x)^T \\phi(z) = (x^Tz)^2$ \n",
    "\n",
    "Proof: $(x^Tz)^2 = (\\sum_{i=1}^n x_iz_i) (\\sum_{i=1}^n x_iz_i) = \\sum_{i=1}^n \\sum_{j=1}^n x_iz_iix_jz_j = \\sum_{i=1}^n \\sum_{j=1}^n (x_ix_j)(z_iz_j)$ Go through every i and j and compute $x_ix_j$ and $z_iz_j$ and add them up. --> $K(x,z) = \\phi(x)^T \\phi(z)$\n",
    "Needs O(n) time to compute $(x^Tz)^2$ instead of O(n^2) time to compute $\\phi(x)^T \\phi(z)$.\n",
    "\n",
    "##### Examples of kernels\n",
    "$K(x,z) = (x^Tz+c)^2$ where c is a constant $c \\in \\mathbb R$. This corresponds to adding a feature to every example. --> \\begin{bmatrix} x_1 & x_1 \\\\ x_1 & x_2 \\\\ ... & ... \\end{bmatrix} --> \\begin{bmatrix} x_1 & x_1 \\\\ x_1 & x_2 \\\\ ... & ...\\\\ \\sqrt{2c} & x_1 \\\\ \\sqrt{2c} & x_2 \\\\ ... & ... \\end{bmatrix}\n",
    "$K(x,z) = (x^Tz+c)^d$ O(n) time $\\phi(x)$ has all \\begin{pmatrix} n+d-1 \\\\ d \\end{pmatrix} features of polynomial of up to degree d.\n",
    "\n",
    "##### Kernels \n",
    "- Kernel properties\n",
    "    - If $x, z$ are \"similar\", $K(x,z) = \\phi(x)^T \\phi(z)$ should be large. (Point in similar direction)\n",
    "    - If $x, z$ are \"dissimilar\", $K(x,z) = \\phi(x)^T \\phi(z)$ should be small. (Point in dissimilar directions)\n",
    "- Kernel functions\n",
    "    - A function can be used as a kernel function if there exists a mapping $\\phi$ such that $K(x,z) = \\phi(x)^T \\phi(z)$. \n",
    "    - Linear Kernel: $K(x,z) = x^Tz$ $\\phi(x) = x$\n",
    "    - Gaussian Kernel: $K(x,z) = exp(-\\frac{\\vert \\vert x-z \\vert \\vert^2}{2 \\sigma^2})$ \n",
    "    - Constraints on $K$ to be a valid kernel function:\n",
    "        - $K(x,z) = \\phi(x)^T\\phi(x) \\geq 0$ for all $x,z$\n",
    "        - Theorem (Mercer): $K$ is a valid kernel function (i.e. $\\exists \\phi$ s.t. $K(x,z) = \\phi(x)^T \\phi(z)$) iff $K$ is symmetric and $K$ is positive semidefinite for any $d$ points.\n",
    "            - Let $\\{x^{(1)}, ..., x^{(d)}\\}$ be $d$ points\n",
    "            - Let $K \\in R^{d \\times d}$ be the kernel matrix where $K_{ij} = K(x^{(i)}, x^{(j)})$ (Kernel function applied to all pairs of points)\n",
    "            - Given any vector $z$, $z^TKz = \\sum_i \\sum_j z_ik_{ij}z_j = \\sum_i \\sum_j z_i \\phi (x^{(i)})^T \\phi (x^{(j)}) z_j = \\sum_i \\sum_j z_i \\sum_k (\\phi (x^{(i)}))_k (\\phi (x^{(j)}))_k z_j = \\sum_k (\\sum_i z_i (\\phi (x^{(i)}))_k)^2 \\geq 0$ (Because it is a sum of squares) --> **$K$ is positive semidefinite.**\n",
    "\n",
    "##### $L_1$ norm soft margin SVM\n",
    "- Normal SVM: $min_{w, b} \\frac{1}{2} \\vert \\vert w \\vert \\vert^2$ s.t. $y^{(i)}(w^Tx^{(i)} + b) \\geq 1$ for $i = 1, ..., m$ (maximize margin subject to constraint that all points are on the correct side of the functional margin)\n",
    "- $L_1$ norm soft margin SVM: $min_{w, b, \\epsilon} \\frac{1}{2} \\vert \\vert w \\vert \\vert^2 + C \\sum_{i=1}^m \\vert \\epsilon_i \\vert$ s.t. $y^{(i)}(w^Tx^{(i)} + b) \\geq 1 - \\epsilon_i$ and $\\epsilon_i \\geq 0$ for $i = 1, ..., m$ (maximize margin subject to constraint that all points are on the correct side of the functional margin and some points are allowed to be on the wrong side of the functional margin)\n",
    "- --> *relaxed constraint*: Wrong side of the functional margin / too close to the functional margin is allowed to some extent\n",
    "- Hyperparameter C: How much you want to penalize the points that are on the wrong side of the functional margin / too close to the functional margin\n",
    "\n",
    "$max \\sum_{i=1}^m \\alpha_i - \\sum_{i=1}^m\\sum_{j=1}^m y^{(i)} y^{(j)} \\alpha_i \\alpha_j \\langle x^{(i)}, x^{(j)} \\rangle$ s.t.  $\\sum_{i=1}^m y^{(i)} \\alpha_i  = 0$ and $C \\geq \\alpha_i \\geq 0$ for $i = 1, ..., m$ (maximize margin subject to constraint that all points are on the correct side of the functional margin and some points are allowed to be on the wrong side of the functional margin) ($C \\geq \\alpha_i \\geq 0$ for $i = 1, ..., m$ is the relaxed constraint)\n",
    "\n",
    "# Principles\n",
    "## Duality\n",
    "Resource: [link](https://www.adeveloperdiary.com/data-science/machine-learning/support-vector-machines-for-beginners-duality-problem/)\n",
    ">In mathematical optimization theory, duality means that optimization problems may be viewed from either of two perspectives, the primal problem or the dual problem (the duality principle). The solution to the dual problem provides a lower bound to the solution of the primal (minimization) problem. ~ Wikipedia\n",
    "\n",
    "In SVM's, minimiizing the primal problem is equivalent to maximizing the dual problem. The dual problem is easier to solve because it is a quadratic programming problem (i.e. a convex optimization problem).\n",
    "Primal problem: $min_{w, b} \\frac{1}{2} \\vert \\vert w \\vert \\vert^2$ s.t. $y^{(i)}(w^Tx^{(i)} + b) \\geq 1$ for $i = 1, ..., m$ \n",
    "Dual problem: $max \\sum_{i=1}^m \\alpha_i - \\sum_{i=1}^m\\sum_{j=1}^m y^{(i)} y^{(j)} \\alpha_i \\alpha_j \\langle x^{(i)}, x^{(j)} \\rangle$ s.t.  $\\sum_{i=1}^m y^{(i)} \\alpha_i  = 0$ and $C \\geq \\alpha_i \\geq 0$ for $i = 1, ..., m$ \n",
    "\n",
    "## Lagrange multiplier\n",
    "Resource: [link](https://www.adeveloperdiary.com/data-science/machine-learning/support-vector-machines-for-beginners-duality-problem/)\n",
    "> In mathematical optimization, the method of Lagrange multipliers is a strategy for finding the local maxima and minima of a function subject to equality constraints. ~ Wikipedia\n",
    "*Lagrangian function*: $L(w, b, \\alpha) = f(w,b)-\\alpha(g(w,b)-c)$ \n",
    "- $f$ objective function\n",
    "- $g$ constraint function\n",
    "- $c$ constraint value\n",
    "- $\\alpha$ is the *Lagrange multiplier*, a coefficient used to find the maxima / minima of a function s.t. constraints\n",
    "Objective: Find $w, b, \\alpha$ so that $\\Delta L = 0$ (i.e. $L$ is at a local maxima / minima, because the derivative of a function at a local maxima / minima is 0)\n",
    "Take derivative w.r.t $\\alpha$: $\\frac{\\partial L}{\\partial \\alpha} = 0 - (g(w,b)-c) = 0$ --> $g(w,b) = c$ (i.e. constraint is satisfied)\n",
    "Take derivative w.r.t $w$: $\\frac{\\partial L}{\\partial w} = 0 - \\alpha \\frac{\\partial g}{\\partial w} = 0$ --> $\\frac{\\partial g}{\\partial w} = 0$ (i.e. constraint is satisfied)\n",
    "Take derivative w.r.t $b$: $\\frac{\\partial L}{\\partial b} = 0 - \\alpha \\frac{\\partial g}{\\partial b} = 0$ --> $\\frac{\\partial g}{\\partial b} = 0$ (i.e. constraint is satisfied)\n",
    "\n",
    "## Multiple constraints\n",
    "For $n$ constraints, there are $D+n$ unknowns e.g. $max_x f(x)$ s.t. $g_1(x) = 0$, $g_2(x) = ...$, $g_n(x)=0$\n",
    "Lagrangian defined as: $L(x_1,..,x_D,\\alpha_1,...,\\alpha_n) = f(x_1,...,x_D) - \\sum_{i=1}^n \\alpha_i g_i(x_1,...,x_D)$\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Tasks\n",
    "- Make sure data points $x, y$ are classified correctly \n",
    "    - $y = 1 \\rightarrow w^Tx + b \\geq 1$ (Positive class must be on the positive side of the hyperplane)\n",
    "    - $y = -1 \\rightarrow w^Tx + b \\leq -1$ (Negative class must be on the negative side of the hyperplane)\n",
    "    - ---> (i.e. $y_i(w^Tx_i + b) \\geq 1$ for all $i$) (Functional margin must be at least 1, which means that the distance between the hyperplane and the closest data points in each direction must be at least 1)\n",
    "- Maximize the margin\n",
    "    - $d = \\frac{2}{\\vert \\vert w \\vert \\vert}$ (Margin to be maximized)\n",
    "    - ---> Minimize $\\vert \\vert w \\vert \\vert$ (Minimize the euclidean norm of w has the same effect as maximizing the margin)\n",
    "    - ---> Minimize $\\frac{1}{2} \\vert \\vert w \\vert \\vert^2$\n",
    "- Euclidean distance between point and hyperplane: $\\frac{\\vert w^Tx + b \\vert}{\\vert \\vert w \\vert \\vert}$\n",
    "\n",
    "- Kernel trick: Slide a kernel over the data points to see how data points relate to each other\n",
    "    - Using the kernel trick, the margin is maximized by $\\text max \\sum_{i=1}^m \\alpha_i - \\sum_{i=1}^m\\sum_{j=1}^m y^{(i)} y^{(j)} \\alpha_i \\alpha_j \\langle x^{(i)}, x^{(j)} \\rangle$ s.t.  $\\sum_{i=1}^m y^{(i)} \\alpha_i  = 0$ and $C \\geq \\alpha_i \\geq 0$ for $i = 1, ..., m$ (maximize margin subject to constraint that all points are on the correct side of the functional margin and some points are allowed to be on the wrong side of the functional margin) ($C \\geq \\alpha_i \\geq 0$ for $i = 1, ..., m$ is the relaxed constraint)\n",
    "    - $\\alpha$ is a vector of Lagrange multipliers. Initialized with zeros.\n",
    "    - $\\langle x^{(i)}, x^{(j)} \\rangle$ is the kernel applied to the data points $x^{(i)}$ and $x^{(j)}$\n",
    "    - $C$ is a hyperparameter that determines how much you want to penalize the points that are on the wrong side of the functional margin / too close to the functional margin\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "from typing import Tuple\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "class OptimalMarginClassifier:\n",
    "    x: np.ndarray\n",
    "    y: np.ndarray\n",
    "\n",
    "    support_vectors: np.ndarray\n",
    "    w: np.ndarray\n",
    "    b: float\n",
    "\n",
    "    def __init__(self, x: np.ndarray, y: np.ndarray):\n",
    "        self.x = x\n",
    "        self.y = y\n",
    "        self.w = np.zeros(x.shape[1])\n",
    "        self.b = 0\n",
    "\n",
    "    def learn(self, C: float = 1, epochs: int = 20, learning_rate: float = 0.001):\n",
    "        \"\"\"\n",
    "        :param x: \n",
    "        :param C: Slack variable determines how much you want to penalize the points that are on the wrong side of the functional margin / too close to the functional margin\n",
    "        :return: \n",
    "        \"\"\"\n",
    "        # Choose w, b and slack, such that: min {1 / 2 * ||w||^2 + C * sum slack_i} s.t. y_i(w^T x_i + b) >= 1 - slack_i\n",
    "        for epoch in range(0, epochs):\n",
    "            support_vectors = []\n",
    "            slack_list = []\n",
    "            for i in range(0, len(self.x)):\n",
    "                # y_i * (w^T x_i + b) >= 1 - C\n",
    "                functional_margin = self.functional_margin(self.x[i], self.y[i])\n",
    "                slack_list.append(self.slack(functional_margin))\n",
    "                    \n",
    "            # 1/2 * ||w||^2 + C * sum slack_i\n",
    "            loss = 0.5 * np.linalg.norm(self.w) ** 2 + C * np.sum(slack_list)\n",
    "            print(f\"Epoch {epoch}: Loss: {loss}\")\n",
    "            \n",
    "            # Update weights & bias \n",
    "            dw = \"TODO\"\n",
    "            self.w = self.w - learning_rate * dw\n",
    "            \n",
    "            db = \"TODO\"\n",
    "            self.b = self.b - learning_rate * db\n",
    "            \n",
    "            # Update support vectors\n",
    "            self.support_vectors = np.array(support_vectors)\n",
    "    \n",
    "    def slack(self, functional_margin: np.ndarray):\n",
    "        \"\"\"\n",
    "        :param functional_margin: The functional margin for the data point.\n",
    "        :return: the slack: max(0, 1 - y_i(w^T x_i + b)). Describes how much the data point is on the wrong side of the functional margin.\n",
    "        \"\"\"\n",
    "        return np.maximum(0, 1 - functional_margin)\n",
    "    \n",
    "    def predict(self, x: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        :param x: Data points.\n",
    "        :return: The labels for the given data points: sign(w^T x + b)\n",
    "        \"\"\"\n",
    "        return np.sign(self.w.T @ x + self.b)\n",
    "\n",
    "    def functional_margin(self, x: np.ndarray, y: float) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        :param x: Data point.\n",
    "        :param y: Label.\n",
    "        :return: The functional margin for a single data point: y * (w^T x + b)\n",
    "        \"\"\"\n",
    "        return y * (self.w.T @ x + self.b)\n",
    "\n",
    "\n",
    "    def plot_decision_boundary(self):\n",
    "        \"\"\"\n",
    "        Plots the decision boundary.\n",
    "        \"\"\"\n",
    "        plt.scatter(self.x[:, 0], self.x[:, 1], c=self.y)\n",
    "        plt.scatter(self.support_vectors[:, 0], self.support_vectors[:, 1], c=\"blue\")\n",
    "        plt.plot(self.support_vectors[:, 0], self.support_vectors[:, 1], c=\"blue\")\n",
    "        plt.show()\n",
    "    "
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Test optimal margin classifier with iris dataset\n",
    "from sklearn import datasets\n",
    "iris = datasets.load_iris()\n",
    "x = iris.data\n",
    "y = iris.target\n",
    "\n",
    "optimal_margin_classifier = OptimalMarginClassifier(x, y)\n",
    "optimal_margin_classifier.learn(epochs=400, learning_rate=0.001, C=1)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "optimal_margin_classifier.plot_decision_boundary()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from typing import Tuple\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "class KernelSupportVectorMachine:\n",
    "    x: np.ndarray\n",
    "    y: np.ndarray\n",
    "\n",
    "    support_vectors: np.ndarray\n",
    "    alpha: np.ndarray\n",
    "    sigma: float\n",
    "\n",
    "    def __init__(self, x: np.ndarray, y: np.ndarray):\n",
    "        self.x = x\n",
    "        self.y = y\n",
    "        self.alpha = np.zeros(len(x))\n",
    "        self.sigma = 1\n",
    "\n",
    "    def learn(self):\n",
    "        pass\n",
    "\n",
    "    def predict(self, x: np.ndarray) -> np.ndarray:\n",
    "        pass\n",
    "\n",
    "    def loss_function(self) -> float:\n",
    "        \"\"\"\n",
    "        Calculates the loss: ∑αi – (1/2) ∑i ∑j αi αj yi yj K(xi, xj) \n",
    "        :return: Loss.\n",
    "        \"\"\"\n",
    "        sum_alpha = 0\n",
    "        for i in range(0, len(self.x)):\n",
    "            sum_alpha += self.alpha[i]\n",
    "\n",
    "        s = 0\n",
    "        for i in range(0, len(self.x)):\n",
    "            for j in range(0, len(self.x)):\n",
    "                s += self.alpha[i] * self.alpha[j] * self.y[i] * self.y[j] * self.gaussian_kernel(self.x[i], self.x[j])\n",
    "\n",
    "        return sum_alpha - (1 / 2) * s\n",
    "\n",
    "    def gaussian_kernel(self, x_i, x_j) -> float:\n",
    "        \"\"\"\n",
    "        Applies the gaussian kernel to two data points: exp(-||x_i - x_j||^2 / (2 * sigma^2))\n",
    "        :param x_i: Data point i (row of x)\n",
    "        :param x_j: Data point j (row of x)\n",
    "        :return: Gaussian kernel applied to x_i and x_j.\n",
    "        \"\"\"\n",
    "        return np.exp(-np.linalg.norm(x_i - x_j) ** 2 / (2 * self.sigma ** 2))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
