{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Support Vector Machine (SVM)\n",
    "Using only numpy\n",
    "Based on [CS229](https://www.youtube.com/watch?v=lDwow4aOrtg) and [CS229](https://www.youtube.com/watch?v=8NYoQiRANpg).\n",
    "> A *support vector machine* can perform binary classification and regression tasks. It uses a hyperplane to separate the data into classes.\n",
    "> The *hyperplane* is chosen to maximize the margin between the classes (maximum margin separators).\n",
    "> *Support vectors* are the data points that are closest to the hyperplane and used to determine the hyperplane.\n",
    "\n",
    "## Notation\n",
    "- Hypothesis: $$h_{w,b}(x) = g(w^Tx + b)$$\n",
    "- Labels: $y \\in \\{-1, 1\\}$\n",
    "- $h$ output values in $\\{-1, 1\\}$\n",
    "- $g(z) = \\begin{cases} 1 & \\text{if } z \\geq 0 \\\\ -1 & otherwise \\end{cases}$ (logistic function)\n",
    "- $w$ is the weight vector without bias, as bias explicitly added as $b$\n",
    "- $w^Tx + b$ is the decision boundary\n",
    "- $g$ is the activation function (logistic function)\n",
    "- $\\gamma$ is the geometric margin\n",
    "- $\\hat{\\gamma}$ is the functional margin\n",
    "(- $\\phi$ is the feature mapping function)\n",
    "- $\\langle x,z \\rangle$ is the inner product of $x$ and $z$: $x^Tz$\n",
    "\n",
    "## Hyperplane\n",
    "- It is always of one dimension less than the data: 2D data -> 1D line, 3D data -> 2D plane, 4D data -> 3D plane, etc.\n",
    "- Functional margin of hyperplane defined by $(w, b)$ with respect to training example $(x^{(i)}, y^{(i)})$: $$\\hat{\\gamma}^{(i)} = y^{(i)}(w^Tx^{(i)} + b)$$\n",
    "- Desired properties:\n",
    "    - $y^{(i)} = 1$, want $w^Tx^{(i)} + b >> 1$ (want the result to be large and positive)\n",
    "    - $y^{(i)} = -1$, want $w^Tx^{(i)} + b << -1$ (want the result to be large and negative)\n",
    "- If $\\hat{\\gamma}^{(i)} > 0$, then $h(x^{(i)}) = y^{(i)}$ (as long as the functional margin is greater than 0, the prediction is correct)\n",
    "- Functional margin with respect to the training set: $\\hat{\\gamma} = \\min_{i=1,...,m} \\hat{j}^{(i)}$ (the smallest functional margin of any of the training examples -> how well the hyperplane separates the data over the entire training set)\n",
    "- Geometric margin of hyperplane defined by $(w, b)$ with respect to training example $(x^{(i)}, y^{(i)})$: $\\gamma^{(i)} = (\\frac{w^Tx^{(i)} + b}{\\vert \\vert w \\vert \\vert}) = \\frac{\\hat{\\gamma}^{(i)}}{\\vert \\vert w \\vert \\vert}$ (the functional margin divided by the Euclidean norm of w)\n",
    "- Geometric margin with respect to the training set: $\\gamma = \\min_{i=1,...,m} \\gamma^{(i)}$ (smallest geometric margin of any of the training examples)\n",
    "\n",
    "## Optimal margin classifier\n",
    "- Choose the parameters $w$ and $b$ to maximize $\\gamma$ (the geometric margin)\n",
    "- $max_{w, b, \\gamma} \\gamma$ subject to $\\hat{\\gamma}^{(i)} \\frac{w^Tx^{(i)} + b}{\\vert \\vert w \\vert \\vert} \\geq \\gamma$ for $i = 1, ..., m$ (the functional margin of any training example must be greater than or equal to the geometric margin)\n",
    "- Can be reformulated to a equivalent problem: $min_{w, b} \\vert \\vert w \\vert \\vert^2$ subject to $y^{(i)}(w^Tx^{(i)} + b) \\geq 1$ for $i = 1, ..., m$ (minimize the euclidean norm of w, subject to the functional margin of any training example being greater than or equal to 1)\n",
    "- Raising the geometric margin is the same as lowering the euclidean norm of w\n",
    "\n",
    "When the $min_i \\gamma^{(i)}$ shall be as large as possible, one possible way is to raise $\\gamma^{(i)} \\geq \\gamma$ for all $i$ (the geometric margin of any training example must be greater than or equal to the geometric margin of the entire training set) --> maximizing the geometric margin has the effect of maximizing the worst case geometric margin\n",
    "\n",
    "Scaling the length of the vectors by a constant factor does not change their direction.\n",
    "--> If $\\vert \\vert w \\vert \\vert$ is scaled such that it is equal to $\\frac{1}{\\gamma}$,\n",
    "the optimization problem becomes $max \\frac{1}{\\vert \\vert w \\vert \\vert}$ subject to $y^{(i)}(w^Tx^{(i)} + b) \\gamma \\geq \\gamma$ = $y^{(i)}(w^Tx^{(i)} + b) \\geq 1$ (maximize the length of w, subject to the functional margin of any training example being greater than or equal to 1)\n",
    "This is equal to $min \\frac{1}{2} \\vert \\vert w \\vert \\vert^2$ subject to $y^{(i)}(w^Tx^{(i)} + b) \\geq 1$ (minimize the euclidean norm of w, subject to the functional margin of any training example being greater than or equal to 1)\n",
    "\n",
    "## Support Vector Machine (SVM)\n",
    "- **Optimal margin classifier + kernel trick (non-linear classification) = support vector machine**\n",
    "    - The optimal margin classifier is a *quadratic programming problem* (can be solved efficiently)\n",
    "    - The *support vector machine* is a *soft margin classifier* by applying the *kernel trick* (non-linear classification)\n",
    "\n",
    "-  Represented theorem: Suppose $w = \\sum_{i=1}^m \\alpha_i x^{(i)}$ (w can be represented as a linear combination of the training examples)\n",
    "    - Why?\n",
    "        - Intuition 1: In logistic regression, the on every iteration the weights are updated by adding the product of the learning rate and the gradient of the cost function with respect to the weights. --> The weights are a linear combination of the training examples.\n",
    "        - Intuition 2: ([video](https://www.youtube.com/watch?v=8NYoQiRANpg&list=PLoROMvodv4rMiGQp3WXShtMGgzqpfVfbU&index=7) at 16:00)\n",
    "\n",
    "Then $min_{w, b} \\frac{1}{2} \\vert \\vert w \\vert \\vert^2$ subject to $y^{(i)}(w^Tx^{(i)} + b) \\geq 1$ for $i = 1, ..., m$ is equivalent to: $min_{\\alpha} \\frac{1}{2} \\sum_{i=1}^m \\sum_{j=1}^m \\alpha_i \\alpha_j y^{(i)} y^{(j)} x^{(i)T} x^{(j)} - \\sum_{i=1}^m \\alpha_i$ subject to $\\alpha_i \\geq 0$ for $i = 1, ..., m$ and $\\sum_{i=1}^m \\alpha_i y^{(i)} = 0$.\n",
    "\n",
    "Plug $w = \\sum_{i=1}^m \\alpha_i x^{(i)}$ into $min_{w, b} \\frac{1}{2} \\vert \\vert w \\vert \\vert^2$ subject to $y^{(i)}(w^Tx^{(i)} + b) \\geq 1$ for $i = 1, ..., m$: $min \\frac{1}{2}(\\sum_{i=1}^m \\alpha_iy^{(i)}x^{(i)})^T(\\sum_{j=1}^m \\alpha_jy^{(j)}x^{(j)})$ = $min \\frac{1}{2} \\sum_{i=1}^m \\sum_{j=1}^m \\alpha_i \\alpha_j y^{(i)} y^{(j)} x^{(i)T} x^{(j)}$\n",
    "The constraint becomes $y^{(i)}(\\sum_{j=1}^m \\alpha_jy^{(j)}x^{(j)})^Tx^{(i)} + b) \\geq 1$ for $i = 1, ..., m$.\n",
    "\n",
    "The optimization Problem can be simplified into the *Dual optimization problem*: $\\sum_i \\alpha_i - \\frac{1}{2} \\sum_i \\sum_j y^{(i)} y^{(j)} \\alpha_i \\alpha_j \\langle x^{(i)}, x^{(j)} \\rangle$ subject to $\\alpha_i \\geq 0$ for $i = 1, ..., m$ and $\\sum_{i=1}^m \\alpha_i y^{(i)} = 0$.\n",
    "--> Solve for $\\alpha_i$'s instead of $w$ and $b$.\n",
    "--> Make predictions using $w = \\sum_{i=1}^m \\alpha_i x^{(i)}$. --> $h_{w,b}(x) = g(w^Tx + b) = g((\\sum_{i=1}^m \\alpha_i x^{(i)})^Tx + b) = g(\\sum_{i=1}^m \\alpha_i \\langle x^{(i)}, x \\rangle + b)$\n",
    "- *Kernel trick*:\n",
    "    - Write algorithm in terms of $\\langle x^{(i)}, x^{(j)} \\rangle$ (or $\\langle x, z \\rangle$ instead of $x^{(i)}$ and $x^{(j)}$.\n",
    "    - Let there be some mapping from $x \\rightarrow \\phi(x)$.\n",
    "    - Find way to compute $K(x, z) = \\phi(x)^T \\phi(z)$ without explicitly computing $\\phi(x)$ and $\\phi(z)$.\n",
    "    - Replace $\\langle x, z \\rangle$ with $K(x, z)$.\n",
    "\n",
    "$K(x,z) = \\phi(x)^T \\phi(z) = (x^Tz)^2$ \n",
    "\n",
    "Proof: $(x^Tz)^2 = (\\sum_{i=1}^n x_iz_i) (\\sum_{i=1}^n x_iz_i) = \\sum_{i=1}^n \\sum_{j=1}^n x_iz_iix_jz_j = \\sum_{i=1}^n \\sum_{j=1}^n (x_ix_j)(z_iz_j)$ Go through every i and j and compute $x_ix_j$ and $z_iz_j$ and add them up. --> $K(x,z) = \\phi(x)^T \\phi(z)$\n",
    "Needs O(n) time to compute $(x^Tz)^2$ instead of O(n^2) time to compute $\\phi(x)^T \\phi(z)$.\n",
    "\n",
    "##### Examples of kernels\n",
    "$K(x,z) = (x^Tz+c)^2$ where c is a constant $c \\in \\mathbb R$. This corresponds to adding a feature to every example. --> \\begin{bmatrix} x_1 & x_1 \\\\ x_1 & x_2 \\\\ ... & ... \\end{bmatrix} --> \\begin{bmatrix} x_1 & x_1 \\\\ x_1 & x_2 \\\\ ... & ...\\\\ \\sqrt{2c} & x_1 \\\\ \\sqrt{2c} & x_2 \\\\ ... & ... \\end{bmatrix}\n",
    "$K(x,z) = (x^Tz+c)^d$ O(n) time $\\phi(x)$ has all \\begin{pmatrix} n+d-1 \\\\ d \\end{pmatrix} features of polynomial of up to degree d.\n",
    "\n",
    "##### Kernels \n",
    "- Kernel properties\n",
    "    - If $x, z$ are \"similar\", $K(x,z) = \\phi(x)^T \\phi(z)$ should be large. (Point in similar direction)\n",
    "    - If $x, z$ are \"dissimilar\", $K(x,z) = \\phi(x)^T \\phi(z)$ should be small. (Point in dissimilar directions)\n",
    "- Kernel functions\n",
    "    - A function can be used as a kernel function if there exists a mapping $\\phi$ such that $K(x,z) = \\phi(x)^T \\phi(z)$. \n",
    "    - Linear Kernel: $K(x,z) = x^Tz$ $\\phi(x) = x$\n",
    "    - Gaussian Kernel: $K(x,z) = exp(-\\frac{\\vert \\vert x-z \\vert \\vert^2}{2 \\sigma^2})$ \n",
    "    - Constraints on $K$ to be a valid kernel function:\n",
    "        - $K(x,z) = \\phi(x)^T\\phi(x) \\geq 0$ for all $x,z$\n",
    "        - Theorem (Mercer): $K$ is a valid kernel function (i.e. $\\exists \\phi$ s.t. $K(x,z) = \\phi(x)^T \\phi(z)$) iff $K$ is symmetric and $K$ is positive semidefinite for any $d$ points.\n",
    "            - Let $\\{x^{(1)}, ..., x^{(d)}\\}$ be $d$ points\n",
    "            - Let $K \\in R^{d \\times d}$ be the kernel matrix where $K_{ij} = K(x^{(i)}, x^{(j)})$ (Kernel function applied to all pairs of points)\n",
    "            - Given any vector $z$, $z^TKz = \\sum_i \\sum_j z_ik_{ij}z_j = \\sum_i \\sum_j z_i \\phi (x^{(i)})^T \\phi (x^{(j)}) z_j = \\sum_i \\sum_j z_i \\sum_k (\\phi (x^{(i)}))_k (\\phi (x^{(j)}))_k z_j = \\sum_k (\\sum_i z_i (\\phi (x^{(i)}))_k)^2 \\geq 0$ (Because it is a sum of squares) --> **$K$ is positive semidefinite.**\n",
    "\n",
    "##### $L_1$ norm soft margin SVM\n",
    "- Normal SVM: $min_{w, b} \\frac{1}{2} \\vert \\vert w \\vert \\vert^2$ s.t. $y^{(i)}(w^Tx^{(i)} + b) \\geq 1$ for $i = 1, ..., m$ (maximize margin subject to constraint that all points are on the correct side of the functional margin)\n",
    "- $L_1$ norm soft margin SVM: $min_{w, b, \\epsilon} \\frac{1}{2} \\vert \\vert w \\vert \\vert^2 + C \\sum_{i=1}^m \\vert \\epsilon_i \\vert$ s.t. $y^{(i)}(w^Tx^{(i)} + b) \\geq 1 - \\epsilon_i$ and $\\epsilon_i \\geq 0$ for $i = 1, ..., m$ (maximize margin subject to constraint that all points are on the correct side of the functional margin and some points are allowed to be on the wrong side of the functional margin)\n",
    "- --> *relaxed constraint*: Wrong side of the functional margin / too close to the functional margin is allowed to some extent\n",
    "- Hyperparameter C: How much you want to penalize the points that are on the wrong side of the functional margin / too close to the functional margin\n",
    "\n",
    "$max \\sum_{i=1}^m \\alpha_i - \\sum_{i=1}^m\\sum_{j=1}^m y^{(i)} y^{(j)} \\alpha_i \\alpha_j \\langle x^{(i)}, x^{(j)} \\rangle$ s.t.  $\\sum_{i=1}^m y^{(i)} \\alpha_i  = 0$ and $C \\geq \\alpha_i \\geq 0$ for $i = 1, ..., m$ (maximize margin subject to constraint that all points are on the correct side of the functional margin and some points are allowed to be on the wrong side of the functional margin) ($C \\geq \\alpha_i \\geq 0$ for $i = 1, ..., m$ is the relaxed constraint)\n",
    "\n",
    "---\n",
    "Article\n",
    "- Distance measure\n",
    "- Margin\n",
    "    - Distance between the hyperplane and the closest data points: $d = \\frac{\\vert w^T (\\phi(x_0)) + b \\vert}{\\vert \\vert w \\vert \\vert}$\n",
    "        - euclidean norm of w: $\\vert \\vert w \\vert \\vert = \\sqrt{w_1^2 + w_2^2 + ... + w_n^2}$\n",
    "- Objective: Find the hyperplane that maximizes the margin (*maximum margin separator*)\n",
    "    - Also: Maximize the minimum distance between the hyperplane and the data points\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "class SupportVectorMachine:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def determine_maximum_margin_separator(self):\n",
    "        pass\n",
    "\n",
    "    def determine_distance_between_hyperplane_and_datapoint(self, datapoint, hyperplane):\n",
    "        \"\"\"\n",
    "        :param datapoint:\n",
    "        :param hyperplane:\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        return np.abs(np.dot(hyperplane, datapoint))\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
