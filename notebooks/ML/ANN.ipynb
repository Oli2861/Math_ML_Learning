{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Artificial Neural Network ANN\n",
    "Objective: MNIST handwritten digits classifications using a Artificial Neural Network (ANN) without the usage of any libraries."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Data loading"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "outputs": [],
   "source": [
    "import random\n",
    "from typing import Callable\n",
    "from abc import ABC\n",
    "from abc import abstractmethod\n",
    "\n",
    "from oli.ml.Activation_functions import relu\n",
    "from oli.math.math_utility import pretty_print_matrix\n",
    "from oli.ml.Loss import mean_squared_error_loss_categorical\n",
    "from oli.ml.Activation_functions import relu_derivative\n",
    "from oli.ml.Loss import derivative_mean_squared_error_loss_categorical"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-03T17:23:50.222277600Z",
     "start_time": "2023-10-03T17:23:49.953278400Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "outputs": [],
   "source": [
    "def softmax(x: list[float]) -> float:\n",
    "    \"\"\"\n",
    "    Converts vector into probability distribution of outcomes.\n",
    "    :param x: Vector.\n",
    "    :return: Probability distribution of outcomes.\n",
    "    \"\"\"\n",
    "    sum = 0.0000000000000000000000001\n",
    "    for item in x:\n",
    "        sum += item\n",
    "    return [curr / sum for curr in x]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-03T17:23:50.223306900Z",
     "start_time": "2023-10-03T17:23:50.018278Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "outputs": [],
   "source": [
    "class Image:\n",
    "    pixels: list[list[int]]\n",
    "    height: int\n",
    "    width: int\n",
    "\n",
    "    def __init__(self, pixels: list[list[int]]):\n",
    "        self.pixels = pixels\n",
    "        self.height = len(pixels)\n",
    "        self.width = len(pixels[0])\n",
    "\n",
    "    def print(self):\n",
    "        pretty_print_matrix(self.pixels, label=f\"Image with dimensions width = {self.width} x height = {self.height}\",\n",
    "                            max_length=3)\n",
    "\n",
    "    def get_linearized(self) -> list[int]:\n",
    "        res: list[int] = []\n",
    "        for row in self.pixels:\n",
    "            for item in row:\n",
    "                res.append(item)\n",
    "        return res"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-03T17:23:50.223306900Z",
     "start_time": "2023-10-03T17:23:50.080278700Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "outputs": [],
   "source": [
    "class MNISTDataset:\n",
    "    images: list[Image]\n",
    "    labels: list[int]\n",
    "\n",
    "    def __init__(self, images: list[Image], labels: list[int]):\n",
    "        self.images = images\n",
    "        self.labels = labels\n",
    "\n",
    "        if len(images) != len(labels):\n",
    "            raise Exception(\"Amount of images doesnt match amount of labels.\")\n",
    "\n",
    "    def __iter__(self):\n",
    "        return ((self.images[i], self.labels[i]) for i in range(len(self.images)))\n",
    "\n",
    "    def get_linearized_images(self) -> list[list[int]]:\n",
    "        return [img.get_linearized() for img in self.images]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-03T17:23:50.252278500Z",
     "start_time": "2023-10-03T17:23:50.145278Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2023-10-03T17:23:50.298306400Z",
     "start_time": "2023-10-03T17:23:50.207277300Z"
    }
   },
   "outputs": [],
   "source": [
    "def read_image_file(path: str) -> list[Image]:\n",
    "    file_stream = open(path, \"rb\")\n",
    "    # Offset 0 - 4 --> 4 bytes\n",
    "    magic_number: bytes = file_stream.read(4)\n",
    "    magic_number: int = int.from_bytes(magic_number, byteorder=\"big\", signed=False)\n",
    "\n",
    "    # Offset 4 - 8 --> 4 bytes\n",
    "    number_of_images: bytes = file_stream.read(4)\n",
    "    number_of_images: int = int.from_bytes(number_of_images, byteorder=\"big\", signed=False)\n",
    "\n",
    "    # Offset 8 - 12 --> 4 bytes\n",
    "    number_of_rows: bytes = file_stream.read(4)\n",
    "    number_of_rows: int = int.from_bytes(number_of_rows, byteorder=\"big\", signed=False)\n",
    "\n",
    "    # Offset 8 - 12 --> 4 bytes\n",
    "    number_of_columns: bytes = file_stream.read(4)\n",
    "    number_of_columns: int = int.from_bytes(number_of_columns, byteorder=\"big\", signed=False)\n",
    "\n",
    "    print(\n",
    "        f\"Loading images:\\tMagic number: {magic_number}, Number of images: {number_of_images}, Number of rows: {number_of_rows}, Number of columns: {number_of_columns}\")\n",
    "\n",
    "    images: list[Image] = []\n",
    "    count = 0\n",
    "    for image_number in range(number_of_images):\n",
    "        pixels: list[list[int]] = [[0 for n in range(number_of_columns)] for i in range(number_of_rows)]\n",
    "        for row_number in range(number_of_rows):\n",
    "            for column_number in range(number_of_columns):\n",
    "                pixel: bytes = file_stream.read(1)\n",
    "                pixel: int = int.from_bytes(pixel, byteorder=\"big\", signed=False)\n",
    "                pixels[row_number][column_number] = pixel\n",
    "        images.append(Image(pixels))\n",
    "        if image_number % 10000 == 0:\n",
    "            print(\"Loaded image number\", image_number)\n",
    "    return images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "outputs": [],
   "source": [
    "def read_label_file(path: str):\n",
    "    file_stream = open(path, \"rb\")\n",
    "    # Offset 0 - 4 --> 4 bytes\n",
    "    magic_number: bytes = file_stream.read(4)\n",
    "    magic_number: int = int.from_bytes(magic_number, byteorder=\"big\", signed=False)\n",
    "\n",
    "    # Offset 4 - 8 --> 4 bytes\n",
    "    number_of_items: bytes = file_stream.read(4)\n",
    "    number_of_items: int = int.from_bytes(number_of_items, byteorder=\"big\", signed=False)\n",
    "\n",
    "    print(f\"Loading labels:\\tMagic number: {magic_number}, Number of items: {number_of_items}\")\n",
    "\n",
    "    items: list[int] = []\n",
    "    for item_index in range(number_of_items):\n",
    "        item: bytes = file_stream.read(1)\n",
    "        item: int = int.from_bytes(item, byteorder=\"big\", signed=False)\n",
    "        items.append(item)\n",
    "\n",
    "    return items"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-03T17:23:50.362277800Z",
     "start_time": "2023-10-03T17:23:50.270278900Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading images:\tMagic number: 2051, Number of images: 60000, Number of rows: 28, Number of columns: 28\n",
      "Loaded image number 0\n",
      "Loaded image number 10000\n",
      "Loaded image number 20000\n",
      "Loaded image number 30000\n",
      "Loaded image number 40000\n",
      "Loaded image number 50000\n",
      "Loading labels:\tMagic number: 2049, Number of items: 60000\n"
     ]
    }
   ],
   "source": [
    "train_images = read_image_file(\"../../data/mnist/train-images.idx3-ubyte\")\n",
    "train_labels = read_label_file(\"../../data/mnist/train-labels.idx1-ubyte\")\n",
    "train_set: MNISTDataset = MNISTDataset(train_images, train_labels)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-03T17:24:10.283395400Z",
     "start_time": "2023-10-03T17:23:50.333279500Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading images:\tMagic number: 2051, Number of images: 10000, Number of rows: 28, Number of columns: 28\n",
      "Loaded image number 0\n",
      "Loading labels:\tMagic number: 2049, Number of items: 10000\n"
     ]
    }
   ],
   "source": [
    "test_images = read_image_file(\"../../data/mnist/t10k-images.idx3-ubyte\")\n",
    "test_labels = read_label_file(\"../../data/mnist/t10k-labels.idx1-ubyte\")\n",
    "test_set: MNISTDataset = MNISTDataset(test_images, test_labels)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-03T17:24:13.601208500Z",
     "start_time": "2023-10-03T17:24:10.286397100Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Utility: Matrix multiplication"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "outputs": [],
   "source": [
    "def multiplication(A: list[list[float]], B: list[list[float]]) -> list[list[float]]:\n",
    "    \"\"\"\n",
    "    Function to multiply two 2d matrices.\n",
    "    :param A: First matrix.\n",
    "    :param B: Second matrix.\n",
    "    :return: Matrix product.\n",
    "    \"\"\"\n",
    "    if len(A[0]) != len(B):\n",
    "        raise Exception(\n",
    "            f\"Multiplication is only possible if the number of columns of A corresponds to the number of rows in B. Columns of A: {len(A[0])} Rows of B: {len(B)}\")\n",
    "    m = len(A)  # Rows of A\n",
    "    n = len(A[0])  # Columns of A\n",
    "    n = len(B)  # Rows of B\n",
    "    p = len(B[0])  # Columns of B\n",
    "    C = [[0 for _ in range(p)] for _ in range(m)]\n",
    "\n",
    "    for y in range(0, m):\n",
    "        for x in range(0, p):\n",
    "            C[y][x] = 0\n",
    "            for u in range(0, n):\n",
    "                a_yu = A[y][u]\n",
    "                b_ux = B[u][x]\n",
    "                # print(f\"i: {i}, j: {j}, u: {u}, a_iu: {a_iu}, b_uj: {b_uj}\")\n",
    "                C[y][x] += a_yu * b_ux\n",
    "\n",
    "    return C"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-03T17:24:13.663209300Z",
     "start_time": "2023-10-03T17:24:13.602210400Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\n",
      "  3.8000000000000003 4.4 5.0 5.6 \n",
      "  8.3 9.8 11.3 12.799999999999999 \n",
      "]\n"
     ]
    }
   ],
   "source": [
    "A = [\n",
    "    [1, 2, 3],\n",
    "    [4, 5, 6]\n",
    "]\n",
    "B = [\n",
    "    [0.1, 0.2, 0.3, 0.4],\n",
    "    [0.5, 0.6, 0.7, 0.8],\n",
    "    [0.9, 1.0, 1.1, 1.2],\n",
    "]\n",
    "pretty_print_matrix(multiplication(A, B))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-03T17:24:13.730227900Z",
     "start_time": "2023-10-03T17:24:13.665209800Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Linear layer\n",
    "- Activation function $g(x)$\n",
    "- Amount of inputs ($x$): $n$ (including bias)\n",
    "- Amount of neurons ($l$): $m$\n",
    "- For each node: Weights for each incoming edge $w_{x_{0..n}}$ --> For all nodes combined $W$ must have dimensionality $n \\times m$\n",
    "\n",
    "## Feed forward\n",
    "Prediction node $l_0$: $x_0 \\cdot w_{l_0}_{x_0} + x_1 \\cdot w_{l_0}_{x_1} + ... + x_n \\cdot w_{l_0}_{x_n}$\n",
    "Prediction node $l_1$: $x_0 \\cdot w_{l_1}_{x_0} + x_1 \\cdot w_{l_1}_{x_1} + ... + x_n \\cdot w_{l_1}_{x_n}$\n",
    "Prediction node $l_2$: $x_0 \\cdot w_{l_1}_{x_0} + x_1 \\cdot w_{l_2}_{x_1} + ... + x_n \\cdot w_{l_2}_{x_n}$\n",
    "--> Feed forward as a matrix multiplication:\n",
    "$$\n",
    "\\hat y = g(\n",
    "\\begin{pmatrix}\n",
    "    x_0\\\\\n",
    "    x_1\\\\\n",
    "    ...\\\\\n",
    "    x_n\n",
    "\\end{pmatrix}\n",
    "\\begin{bmatrix}\n",
    "\tw_{l_0, x_0} & w_{l_1, x_0} & ... & w_{l_m, x_0} \\\\\n",
    "\tw_{l_0, x_1} & w_{l_1, x_1} & ... & w_{l_m, x_1} \\\\\n",
    "\t\\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
    "\tw_{l_0, x_n} & w_{l_1, x_n} & ...& w_{l_m, x_n}\n",
    "\\end{bmatrix})\n",
    "$$\n",
    "\n",
    "## Backpropagation\n",
    "[3b1b explanation](https://www.3blue1brown.com/lessons/backpropagation-calculus)\n",
    "\n",
    "### Symbols & coding\n",
    "| Symbol                | Meaning                                                            |\n",
    "|-----------------------|--------------------------------------------------------------------|\n",
    "| $w$                   | Weight                                                             |\n",
    "| $b$                   | Bias                                                               |\n",
    "| $z$                   | Matrix multiplication product                                      |\n",
    "| $a$                   | Activation                                                         |\n",
    "| $C$                   | Total cost of network (Average of costs for each training example) |\n",
    "| $C_0$ | Cost of sample                                                     |\n",
    "\n",
    "|Indices| Meaning                     |\n",
    "|-------|-----------------------------|\n",
    "| $j$| Current neuron of layer L   |\n",
    "| $k$| Current neuron of layer L-1 |\n",
    "\n",
    "### Objective\n",
    "Goal: How sensitive is cost $C_0$ to changes in $w^{(L)}$: $\\frac{\\partial C_0}{\\partial w^{(L)}}$\n",
    "Sensitivity given by the chain rule\n",
    "\n",
    "### Chain rule\n",
    "$$\\frac{\\partial C_0}{\\partial w^{(L)}} = \\frac{\\partial z^{(L)}}{\\partial w^{(L)}} \\frac{\\partial a^{(L)}}{\\partial z^{(L)}} \\frac{\\partial C_0}{\\partial a^{(L)}} = a^{(L-1)}g'(z^{(L)})2(a^{(L)}-y)$$\n",
    "\n",
    "##### Chain rule components\n",
    "| Description                                                  | Formula                                                   | Per neuron                                                            |\n",
    "|--------------------------------------------------------------|-----------------------------------------------------------|-----------------------------------------------------------------------|\n",
    "| How much do changes in $w^{(L)}$ affect changes in $z^{(L)}$ | $\\frac{\\partial z^{(L)}}{\\partial w^{(L)}} = a^{(L-1)}$   | $\\frac{\\partial z_j^{(L)}}{\\partial w_{ji}^{(L)}} = a_i^{(L-1)}$      |\n",
    "| How much do changes in $z^{(L)}$ affect changes in $a^{(L)}$ | $\\frac{\\partial a^{(L)}}{\\partial z^{(L)}} = g'(z^{(L)})$ | $\\frac{\\partial a_{j}^{(L)}}{\\partial z_{j}^{(L)}} = g'(z_{j}^{(L)})$ |\n",
    "| How much do changes in $a^{(L)}$ affect changes in $C_0$     | $\\frac{\\partial C_0}{\\partial a^{(L)}} = 2(a^{(L)} - y)$  | $\\frac{\\partial C_0}{\\partial a_j^{(L)}} = 2(a_j^{(L)} - y)$          |\n",
    "| Activation                                                   | $a^{(L)} = g(z^{(L)}) = g(w^{(L)} \\cdot a^{(L-1)})$       |                                                                       |\n",
    "\n",
    "### Upstream error calculation:\n",
    "\n",
    "| Description                                                                                                                                                  | Formula                                                                                                                                                                                                   |\n",
    "|--------------------------------------------------------------------------------------------------------------------------------------------------------------|-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|\n",
    "| Previous neuron influences multiple neurons in the following layer --> Sum the error up by summing up the chain rule expressions (one per path of influence) | $\\frac{\\partial C_0}{\\partial a_k^{(L-1)}} = \\sum_{j=0}^{n_{L}} \\frac{\\partial z_j^{(L)}}{\\partial a_k^{(L-1)}} \\frac{\\partial a_j^{(L)}}{\\partial z_j^{(L)}} \\frac{\\partial C_0}{\\partial a_j^{(L)}}$  |\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "outputs": [],
   "source": [
    "class Layer(ABC):\n",
    "\n",
    "    @abstractmethod\n",
    "    def forward(self, x: list[list[float]]) -> list[list[float]]:\n",
    "        pass\n",
    "\n",
    "    # @abstractmethod\n",
    "    # def backprop(self, previous_activation: list[float], label: float, learning_rate: float):\n",
    "    #     pass"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-03T17:29:25.843184700Z",
     "start_time": "2023-10-03T17:29:25.684155700Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Loss:0.12063121928780245\tLabel: 5\n",
      "Prediction: [0.0, 0.0, 0.29896905786194855, 0.18436898694573928, 0.028442063578037587, 0.0, 0.0, 0.0, 0.2753308499200946, 0.21288904169418008]\n",
      "Activation: [0, 0, 221136.7812874266, 136371.18380735774, 21037.583078994914, 0, 0, 0, 203652.4394058724, 157466.4541818713]\n",
      "\n",
      "\n",
      "Loss:0.13845804558053973\tLabel: 0\n",
      "Prediction: [0.0, 0.0, 0.0, 0.0, 0.0, 0.11278531017795707, 0.0, 0.5365723294359697, 0.3506423603860732, 0.0]\n",
      "Activation: [0, 0, 0, 0, 0, 7.881080335380973e+24, 0, 3.749397530010836e+25, 2.450177781867147e+25, 0]\n",
      "\n",
      "\n",
      "Loss:0.14099720588189119\tLabel: 4\n",
      "Prediction: [0.018248436690044917, 0.0, 0.0, 0.0, 0.0, 0.33133580996716033, 0.5775323783675756, 0.0, 0.07288337497521914, 0.0]\n",
      "Activation: [3.381830249168394e+116, 0, 0, 0, 0, 6.140369631722658e+117, 1.0702924859876454e+118, 0, 1.3506866716267307e+117, 0]\n",
      "\n",
      "\n",
      "Loss:nan\tLabel: 1\n",
      "Prediction: [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan]\n",
      "Activation: [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan]\n",
      "\n",
      "\n",
      "Loss:nan\tLabel: 9\n",
      "Prediction: [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan]\n",
      "Activation: [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan]\n",
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[80], line 242\u001B[0m\n\u001B[0;32m    208\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m predictions\n\u001B[0;32m    211\u001B[0m nn \u001B[38;5;241m=\u001B[39m NeuralNetwork(\n\u001B[0;32m    212\u001B[0m     LinearLayer(\n\u001B[0;32m    213\u001B[0m         neurons\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m256\u001B[39m,\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m    239\u001B[0m     )\n\u001B[0;32m    240\u001B[0m )\n\u001B[1;32m--> 242\u001B[0m \u001B[43mnn\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtrain\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m    243\u001B[0m \u001B[43m    \u001B[49m\u001B[43mX\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43m \u001B[49m\u001B[43mtrain_set\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mget_linearized_images\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    244\u001B[0m \u001B[43m    \u001B[49m\u001B[43my\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43m \u001B[49m\u001B[43mtrain_set\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mlabels\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    245\u001B[0m \u001B[43m    \u001B[49m\u001B[43mepochs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m20\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[0;32m    246\u001B[0m \u001B[43m    \u001B[49m\u001B[43mbatch_size\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m1000\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[0;32m    247\u001B[0m \u001B[43m    \u001B[49m\u001B[43mlearning_rate\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m0.001\u001B[39;49m\n\u001B[0;32m    248\u001B[0m \u001B[43m)\u001B[49m\n",
      "Cell \u001B[1;32mIn[80], line 177\u001B[0m, in \u001B[0;36mNeuralNetwork.train\u001B[1;34m(self, X, y, epochs, batch_size, learning_rate)\u001B[0m\n\u001B[0;32m    175\u001B[0m probs: \u001B[38;5;28mfloat\u001B[39m \u001B[38;5;241m=\u001B[39m softmax(pred)\n\u001B[0;32m    176\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124mLoss:\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mmean_squared_error_loss_categorical(probs,\u001B[38;5;250m \u001B[39my[index])\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;130;01m\\t\u001B[39;00m\u001B[38;5;124mLabel: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mcurr_y\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124mPrediction: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mprobs\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124mActivation: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mpred\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m--> 177\u001B[0m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbackprop\u001B[49m\u001B[43m(\u001B[49m\u001B[43mlearning_rate\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcurr_x\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcurr_y\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    178\u001B[0m count \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m \u001B[38;5;241m1\u001B[39m\n\u001B[0;32m    179\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m count \u001B[38;5;241m==\u001B[39m \u001B[38;5;241m1000\u001B[39m: \u001B[38;5;28;01mreturn\u001B[39;00m\n",
      "Cell \u001B[1;32mIn[80], line 184\u001B[0m, in \u001B[0;36mNeuralNetwork.backprop\u001B[1;34m(self, learning_rate, x, y)\u001B[0m\n\u001B[0;32m    182\u001B[0m activation_cost_effect \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[0;32m    183\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m index \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mreversed\u001B[39m(\u001B[38;5;28mrange\u001B[39m(\u001B[38;5;28mlen\u001B[39m(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mlayers))):\n\u001B[1;32m--> 184\u001B[0m     activation_cost_effect \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mlayers\u001B[49m\u001B[43m[\u001B[49m\u001B[43mindex\u001B[49m\u001B[43m]\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbackprop\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m    185\u001B[0m \u001B[43m        \u001B[49m\u001B[43mprevious_activation\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mx\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mif\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mindex\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m==\u001B[39;49m\u001B[43m \u001B[49m\u001B[38;5;241;43m0\u001B[39;49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01melse\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mlayers\u001B[49m\u001B[43m[\u001B[49m\u001B[43mindex\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m-\u001B[39;49m\u001B[43m \u001B[49m\u001B[38;5;241;43m1\u001B[39;49m\u001B[43m]\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mactivation_a\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    186\u001B[0m \u001B[43m        \u001B[49m\u001B[43mlearning_rate\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mlearning_rate\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    187\u001B[0m \u001B[43m        \u001B[49m\u001B[43mlabel\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43my\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    188\u001B[0m \u001B[43m        \u001B[49m\u001B[43mactivation_cost_effect\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mactivation_cost_effect\u001B[49m\n\u001B[0;32m    189\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n",
      "Cell \u001B[1;32mIn[80], line 94\u001B[0m, in \u001B[0;36mLinearLayer.backprop\u001B[1;34m(self, previous_activation, learning_rate, label, activation_cost_effect, print_info)\u001B[0m\n\u001B[0;32m     89\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m neuron_index_j \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mrange\u001B[39m(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mneurons):\n\u001B[0;32m     90\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m previous_activation_index_k \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mrange\u001B[39m(\u001B[38;5;28mlen\u001B[39m(previous_activation)):\n\u001B[0;32m     91\u001B[0m \n\u001B[0;32m     92\u001B[0m         \u001B[38;5;66;03m# Calculate chain rule components\u001B[39;00m\n\u001B[0;32m     93\u001B[0m         \u001B[38;5;66;03m# Effect of a weight change on the matrix multiplication product\u001B[39;00m\n\u001B[1;32m---> 94\u001B[0m         w_on_z_effect: \u001B[38;5;28mfloat\u001B[39m \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43meffect_of_weights_on_matrix_multiplication_product\u001B[49m\u001B[43m(\u001B[49m\u001B[43mprevious_activation\u001B[49m\u001B[43m[\u001B[49m\u001B[43mneuron_index_j\u001B[49m\u001B[43m]\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     96\u001B[0m         \u001B[38;5;66;03m# Effect of matrix multiplication product change on activation\u001B[39;00m\n\u001B[0;32m     97\u001B[0m         z_on_a_effect: \u001B[38;5;28mfloat\u001B[39m \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39meffect_of_matrix_multiplication_product_on_activation(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mmatrix_multiplication_result_z[neuron_index_j])\n",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "class LinearLayer(Layer):\n",
    "    activation_function: Callable[[float], float]\n",
    "    derivative_activation_function: Callable[[float], float]\n",
    "    derivative_cost_function: Callable[[float, float], float]\n",
    "    bias: float = 1\n",
    "    W = list[list[float]]\n",
    "    neurons: int\n",
    "    inputs: int\n",
    "    test_mode: bool\n",
    "\n",
    "    # For backpropagation\n",
    "    activation_a: list[float]\n",
    "    matrix_multiplication_result_z: list[float]\n",
    "\n",
    "    def __init__(\n",
    "            self,\n",
    "            neurons: int,\n",
    "            inputs: int,\n",
    "            activation_function: Callable[[float], float],\n",
    "            derivative_activation_function: Callable[[float], float],\n",
    "            derivative_cost_function: Callable[[float, float], float],\n",
    "            test_mode: bool = False\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Create a linear layer with a fixed number of neurons for a fixed number of inputs.\n",
    "        :param neurons: The number of neurons the linear layer shall contain.\n",
    "        :param inputs: The number of inputs.\n",
    "        :param activation_function: The activation function of the linear layer.\n",
    "        \"\"\"\n",
    "        self.neurons = neurons\n",
    "        self.inputs = inputs\n",
    "        self.activation_function = activation_function\n",
    "        self.derivative_activation_function = derivative_activation_function\n",
    "        self.derivative_cost_function = derivative_cost_function\n",
    "        self.test_mode = test_mode\n",
    "\n",
    "        if test_mode:\n",
    "            self.W = [[0.5 for m in range(neurons)] for n in range((inputs + 1))]\n",
    "        else:\n",
    "            self.W = [[random.random() * 2 - 1 for m in range(neurons)] for n in range((inputs + 1))]\n",
    "\n",
    "        assert len(self.W) == inputs + 1\n",
    "        assert len(self.W[0]) == neurons\n",
    "\n",
    "    def forward(self, x: list[float]) -> list[list[float]]:\n",
    "        \"\"\"\n",
    "        Forward pass through the linear layer by multiplying the weights (including bias) with the data (padded by a additional unit for the bias).\n",
    "        :param x: Data used to make a prediction.\n",
    "        :return: Prediction of the linear layer.\n",
    "        \"\"\"\n",
    "        # Add bias to input to allow the forward pass to be treated as a matrix multiplication.\n",
    "        assert len(x) == self.inputs\n",
    "        x.insert(0, self.bias)\n",
    "\n",
    "        if self.test_mode: print(f\"Input count: {len(x):^6}\\tWeight dimensions count: {len(self.W):^6} x {len(self.W[0]):^6} = {len(self.W) * len(self.W[0])}\")\n",
    "\n",
    "        x: list[list[float]] = [x]\n",
    "        # Multiply weights with input (bias inserted into inputs)\n",
    "        multi: list[float] = multiplication(x, self.W)[0]\n",
    "        assert len(multi) == self.neurons\n",
    "        # Save result of the multiplication (z) for backpropagation\n",
    "        self.matrix_multiplication_result_z = multi\n",
    "\n",
    "        # Apply activation function\n",
    "        activation = [self.activation_function(curr) for curr in multi]\n",
    "        # Save activation (a) for backpropagation\n",
    "        self.activation_a = activation\n",
    "\n",
    "        return activation\n",
    "\n",
    "    def backprop(\n",
    "            self,\n",
    "            previous_activation: list[float],\n",
    "            learning_rate: float,\n",
    "            label: int | None = None,\n",
    "            activation_cost_effect: list[float] | None = None,\n",
    "            print_info: bool = False\n",
    "    ) -> list[float]:\n",
    "        \"\"\"\n",
    "        Backpropagate the error through the neural network. Adjust the weights based on the learning rate and the error received at the corresponding linear layer.\n",
    "        :param previous_activation: Activation received from the previous layer / input: a^{(L-1)}\n",
    "        :param label:\n",
    "        :param learning_rate: Describes how large the gradient steps are.\n",
    "        :param activation_cost_effect: Costs induced by the activation of this layer. If this layer is the last layer set to None in order to calculate the effect based on the loss.\n",
    "        :return: Activation cost effect of the upstream layer.\n",
    "        \"\"\"\n",
    "        activation_cost_effect_for_upstream_layer = [0 for index in range(len(previous_activation))]\n",
    "\n",
    "        for neuron_index_j in range(self.neurons):\n",
    "            for previous_activation_index_k in range(len(previous_activation)):\n",
    "\n",
    "                # Calculate chain rule components\n",
    "                # Effect of a weight change on the matrix multiplication product\n",
    "                w_on_z_effect: float = self.effect_of_weights_on_matrix_multiplication_product(previous_activation[neuron_index_j])\n",
    "\n",
    "                # Effect of matrix multiplication product change on activation\n",
    "                z_on_a_effect: float = self.effect_of_matrix_multiplication_product_on_activation(self.matrix_multiplication_result_z[neuron_index_j])\n",
    "\n",
    "                prev_a_on_z_effect: float = self.effect_of_previous_activation_on_matrix_product(neuron_index_j, previous_activation_index_k)\n",
    "\n",
    "                if neuron_index_j == 0 and print_info:\n",
    "                    print(\"prev_a_on_z_effect\", prev_a_on_z_effect)\n",
    "                    print(\"z_on_a_effect\", z_on_a_effect)\n",
    "\n",
    "                # Effect of activation change on costs\n",
    "                if activation_cost_effect is None and label is not None:\n",
    "                    a_on_c0_effect: float = self.effect_of_activation_on_cost(self.activation_a[neuron_index_j], label)\n",
    "                    if neuron_index_j == 0 and print_info:\n",
    "                        print(\"a_on_c0_effect (based on label)\", a_on_c0_effect)\n",
    "                elif activation_cost_effect is not None:\n",
    "                    a_on_c0_effect: float =  activation_cost_effect[neuron_index_j]\n",
    "                    if neuron_index_j == 0 and print_info:\n",
    "                        print(\"a_on_c0_effect (based on previous activation)\", a_on_c0_effect)\n",
    "                else:\n",
    "                    raise Exception(\"Illegal state.\")\n",
    "\n",
    "                # Effect of previous activation\n",
    "                activation_cost_effect_for_upstream_layer[previous_activation_index_k] += prev_a_on_z_effect * z_on_a_effect * a_on_c0_effect\n",
    "\n",
    "                # Chain rule: Effect of changes of the weights on the costs\n",
    "                cost_sensitivity_with_respect_to_weight_changes = w_on_z_effect * z_on_a_effect * a_on_c0_effect\n",
    "\n",
    "                # Adjust weights\n",
    "                self.W[previous_activation_index_k][neuron_index_j] = self.W[previous_activation_index_k][neuron_index_j] - learning_rate * cost_sensitivity_with_respect_to_weight_changes\n",
    "\n",
    "        return activation_cost_effect_for_upstream_layer\n",
    "\n",
    "    def effect_of_weights_on_matrix_multiplication_product(self, previous_activation: float) -> float:\n",
    "        \"\"\"\n",
    "        The effect of the weights is given by the previous activation a^{(L-1)}.\n",
    "        \"\"\"\n",
    "        return previous_activation\n",
    "\n",
    "    def effect_of_matrix_multiplication_product_on_activation(self, matrix_multiplication_result_z_j: float) -> float:\n",
    "        \"\"\"\n",
    "        The effect of the matrix multiplication product on the activation is the derivative of the activation function applied to the matrix multiplication product: \\sigma ' (z^{(L)}).\n",
    "        \"\"\"\n",
    "        return self.derivative_activation_function(matrix_multiplication_result_z_j)\n",
    "\n",
    "    def effect_of_activation_on_cost(self, activation: float, label: float):\n",
    "        \"\"\"\n",
    "        The effect of the activation on the cost is the derivative of the loss function. e.g. 2(a^{(L)} -y) for squared error loss\n",
    "        \"\"\"\n",
    "        return self.derivative_cost_function(activation, label)\n",
    "\n",
    "    def effect_of_previous_activation_on_matrix_product(self, neuron_index_j: int, previous_activation_index_k: int) -> float:\n",
    "        \"\"\"\n",
    "        Calculate the effect of the previous activation on the matrix multiplication product.\n",
    "        It is the activation at the index of the relevant weight: Row index in the weight matrix denotes the feature index, column index denotes the neurons of the layer.\n",
    "        \"\"\"\n",
    "        return self.W[previous_activation_index_k][neuron_index_j]\n",
    "\n",
    "\n",
    "class NeuralNetwork:\n",
    "    layers: list[LinearLayer]\n",
    "\n",
    "    def __init__(self, *args):\n",
    "        self.layers = args\n",
    "\n",
    "    def train(self, X: list[list[float]], y: list[int], epochs: int, batch_size: int, learning_rate: float):\n",
    "        \"\"\"\n",
    "        :param X: Data to train on.\n",
    "        :param y: Labels corresponding to the data.\n",
    "        :param epochs: The number of epochs to train the NN for.\n",
    "        :param batch_size: Size of a training batch.\n",
    "        :param learning_rate: The learning rate (amount by which the weights are adjusted).\n",
    "        \"\"\"\n",
    "        count = 0\n",
    "        for epoch in range(epochs):\n",
    "            for index in range(len(X)):\n",
    "                curr_x: list[float] = X[index]\n",
    "                curr_y = y[index]\n",
    "                # Prediction itself not used, as the activations of each layer are accessed directly\n",
    "                pred: float = self.predict(curr_x)\n",
    "                probs: float = softmax(pred)\n",
    "                print(f\"\\nLoss:{mean_squared_error_loss_categorical(probs, y[index])}\\tLabel: {curr_y}\\nPrediction: {probs}\\nActivation: {pred}\\n\")\n",
    "                self.backprop(learning_rate, curr_x, curr_y)\n",
    "                count += 1\n",
    "                if count == 1000: return\n",
    "\n",
    "    def backprop(self, learning_rate: float, x: list[float], y: int):\n",
    "        activation_cost_effect = None\n",
    "        for index in reversed(range(len(self.layers))):\n",
    "            activation_cost_effect = self.layers[index].backprop(\n",
    "                previous_activation=x if index == 0 else self.layers[index - 1].activation_a,\n",
    "                learning_rate=learning_rate,\n",
    "                label=y,\n",
    "                activation_cost_effect=activation_cost_effect\n",
    "            )\n",
    "\n",
    "    def predict(self, x: list[float]) -> list[float]:\n",
    "        curr_x: list[float] = x\n",
    "        log = False\n",
    "        for (index, layer) in enumerate(self.layers):\n",
    "            if index == 3 and log:\n",
    "                pretty_print_matrix(curr_x, \"X: \")\n",
    "                pretty_print_matrix(layer.W, \"Weights: \")\n",
    "            curr_x = layer.forward(curr_x)\n",
    "            if index == 0 and log:\n",
    "                pretty_print_matrix(curr_x, \"Activation: \")\n",
    "\n",
    "        return curr_x\n",
    "\n",
    "    def predict_multiple(self, X: list[list[float]]) -> list[float]:\n",
    "        predictions: list[float] = []\n",
    "        for x in X:\n",
    "            predictions.append(self.predict(x))\n",
    "        return predictions\n",
    "\n",
    "\n",
    "nn = NeuralNetwork(\n",
    "    LinearLayer(\n",
    "        neurons=256,\n",
    "        inputs=784,\n",
    "        activation_function=relu,\n",
    "        derivative_activation_function=relu_derivative,\n",
    "        derivative_cost_function=derivative_mean_squared_error_loss_categorical\n",
    "    ),\n",
    "    LinearLayer(\n",
    "        neurons=128,\n",
    "        inputs=256,\n",
    "        activation_function=relu,\n",
    "        derivative_activation_function=relu_derivative,\n",
    "        derivative_cost_function=derivative_mean_squared_error_loss_categorical\n",
    "    ),\n",
    "    LinearLayer(\n",
    "        neurons=64,\n",
    "        inputs=128,\n",
    "        activation_function=relu,\n",
    "        derivative_activation_function=relu_derivative,\n",
    "        derivative_cost_function=derivative_mean_squared_error_loss_categorical\n",
    "    ),\n",
    "    LinearLayer(\n",
    "        neurons=10,\n",
    "        inputs=64,\n",
    "        activation_function=relu,\n",
    "        derivative_activation_function=relu_derivative,\n",
    "        derivative_cost_function=derivative_mean_squared_error_loss_categorical\n",
    "    )\n",
    ")\n",
    "\n",
    "nn.train(\n",
    "    X = train_set.get_linearized_images(),\n",
    "    y= train_set.labels,\n",
    "    epochs=20,\n",
    "    batch_size=1000,\n",
    "    learning_rate=0.001\n",
    ")\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-03T17:40:31.705533600Z",
     "start_time": "2023-10-03T17:40:27.058877500Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input count:   4   \tWeight dimensions count:   4    x   4    = 16\n",
      "[\n",
      "  3.5 \n",
      "  3.5 \n",
      "  3.5 \n",
      "  3.5 \n",
      "]\n",
      "10.7101\n"
     ]
    }
   ],
   "source": [
    "layer = LinearLayer(neurons=4, inputs=3, activation_function=relu, derivative_activation_function=relu_derivative, derivative_cost_function=derivative_mean_squared_error_loss_categorical, test_mode=True)\n",
    "result = layer.forward([1, 2, 3])\n",
    "assert result == [3.5, 3.5, 3.5, 3.5]\n",
    "pretty_print_matrix(result)\n",
    "\n",
    "loss = mean_squared_error_loss_categorical(result, 2)\n",
    "assert loss == 10.7101\n",
    "print(loss)\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-03T17:24:18.636147100Z",
     "start_time": "2023-10-03T17:24:18.514610300Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input count:  785  \tWeight dimensions count:  785   x  256   = 200960\n",
      "Input count:  257  \tWeight dimensions count:  257   x  128   = 32896\n",
      "Input count:  129  \tWeight dimensions count:  129   x   64   = 8256\n",
      "Input count:   65  \tWeight dimensions count:   65   x   10   = 650\n",
      "Image with dimensions width = 28 x height = 28\n",
      "[\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0  \n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0  \n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0  \n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0  \n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0  \n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0  \n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0  \n",
      "   0   0   0   0   0   0  84  185 159 151 60  36   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0  \n",
      "   0   0   0   0   0   0  222 254 254 254 254 241 198 198 198 198 198 198 198 198 170 52   0   0   0   0   0   0  \n",
      "   0   0   0   0   0   0  67  114 72  114 163 227 254 225 254 254 254 250 229 254 254 140  0   0   0   0   0   0  \n",
      "   0   0   0   0   0   0   0   0   0   0   0  17  66  14  67  67  67  59  21  236 254 106  0   0   0   0   0   0  \n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0  83  253 209 18   0   0   0   0   0   0  \n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0  22  233 255 83   0   0   0   0   0   0   0  \n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0  129 254 238 44   0   0   0   0   0   0   0  \n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0  59  249 254 62   0   0   0   0   0   0   0   0  \n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0  133 254 187  5   0   0   0   0   0   0   0   0  \n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   9  205 248 58   0   0   0   0   0   0   0   0   0  \n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0  126 254 182  0   0   0   0   0   0   0   0   0   0  \n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0  75  251 240 57   0   0   0   0   0   0   0   0   0   0  \n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0  19  221 254 166  0   0   0   0   0   0   0   0   0   0   0  \n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   3  203 254 219 35   0   0   0   0   0   0   0   0   0   0   0  \n",
      "   0   0   0   0   0   0   0   0   0   0   0   0  38  254 254 77   0   0   0   0   0   0   0   0   0   0   0   0  \n",
      "   0   0   0   0   0   0   0   0   0   0   0  31  224 254 115  1   0   0   0   0   0   0   0   0   0   0   0   0  \n",
      "   0   0   0   0   0   0   0   0   0   0   0  133 254 254 52   0   0   0   0   0   0   0   0   0   0   0   0   0  \n",
      "   0   0   0   0   0   0   0   0   0   0  61  242 254 254 52   0   0   0   0   0   0   0   0   0   0   0   0   0  \n",
      "   0   0   0   0   0   0   0   0   0   0  121 254 254 219 40   0   0   0   0   0   0   0   0   0   0   0   0   0  \n",
      "   0   0   0   0   0   0   0   0   0   0  121 254 207 18   0   0   0   0   0   0   0   0   0   0   0   0   0   0  \n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0  \n",
      "]\n",
      "[\n",
      "  0.10000000000000002 \n",
      "  0.10000000000000002 \n",
      "  0.10000000000000002 \n",
      "  0.10000000000000002 \n",
      "  0.10000000000000002 \n",
      "  0.10000000000000002 \n",
      "  0.10000000000000002 \n",
      "  0.10000000000000002 \n",
      "  0.10000000000000002 \n",
      "  0.10000000000000002 \n",
      "]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "nn = NeuralNetwork(\n",
    "    LinearLayer(\n",
    "        neurons=256,\n",
    "        inputs=784,\n",
    "        activation_function=relu,\n",
    "        derivative_activation_function=relu_derivative,\n",
    "        derivative_cost_function=derivative_mean_squared_error_loss_categorical,\n",
    "        test_mode=True\n",
    "    ),\n",
    "    LinearLayer(\n",
    "        neurons=128,\n",
    "        inputs=256,\n",
    "        activation_function=relu,\n",
    "        derivative_activation_function=relu_derivative,\n",
    "        derivative_cost_function=derivative_mean_squared_error_loss_categorical,\n",
    "        test_mode=True\n",
    "    ),\n",
    "    LinearLayer(\n",
    "        neurons=64,\n",
    "        inputs=128,\n",
    "        activation_function=relu,\n",
    "        derivative_activation_function=relu_derivative,\n",
    "        derivative_cost_function=derivative_mean_squared_error_loss_categorical,\n",
    "        test_mode=True\n",
    "    ),\n",
    "    LinearLayer(\n",
    "        neurons=10,\n",
    "        inputs=64,\n",
    "        activation_function=relu,\n",
    "        derivative_activation_function=relu_derivative,\n",
    "        derivative_cost_function=derivative_mean_squared_error_loss_categorical,\n",
    "        test_mode=True\n",
    "    )\n",
    ")\n",
    "\n",
    "prediction = softmax(\n",
    "    nn.predict(test_set.images[0].get_linearized())\n",
    ")\n",
    "test_set.images[0].print()\n",
    "pretty_print_matrix(softmax(prediction))\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-03T17:24:18.699149100Z",
     "start_time": "2023-10-03T17:24:18.578149Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-03T17:24:18.706148500Z",
     "start_time": "2023-10-03T17:24:18.687149400Z"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
