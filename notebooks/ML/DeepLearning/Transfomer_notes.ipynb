{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2023-10-06T08:37:16.801875002Z",
     "start_time": "2023-10-06T08:37:16.755088380Z"
    }
   },
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Self-attention\n",
    "Based on [Peter Bloem's blog](https://peterbloem.nl/blog/transformers).\n",
    ">Self-attention is a sequence-to-sequence operation taking in $x_1,...,x_t$ and returning $y_1,...,y_t$ where each $y_i$ is the weighted sum of all $x_{ij}$'s: $y_i = \\sum_j w_{ij}x_j$\n",
    ">**Intuition:** \n",
    "> - The dot product expresses how related two vectors of the input sequence are. \n",
    "> - Related is defined by the learning task.\n",
    "> - The output vectors are weighted sums of the input vectors, where the weights are calculated by the dot products.\n",
    "- $w_{ij}$ is not a parameter but derived from a function of $x_i$ and $x_j$.\n",
    "- Simplest function is dot product: $w_{ij} = x_i^Tx_j$. As this yields results in $[-\\infty, \\infty]$, the softmax function is applied to get a probability distribution: $w_{ij} = \\frac{exp(x_i^Tx_j)}{\\sum_{j'}exp(x_i^Tx_{j'})}$\n",
    "\n",
    "Input: Sequence of $t$ vectors of $k$ dimensions: $X^{(t,k)}$\n",
    "With minibatch dimension $b$: $X^{(b,t,k)}$"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "6210e535676bbeaa"
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x: \n",
      " tensor([[[-0.6576, -0.0910,  0.6779,  1.7254],\n",
      "         [ 0.7237, -0.8033,  0.9599, -1.4178],\n",
      "         [-0.3415, -0.3925, -0.8440,  0.2096]],\n",
      "\n",
      "        [[-0.7420, -1.5567, -2.0906, -0.9844],\n",
      "         [ 1.1749,  0.9946, -0.6373,  0.4512],\n",
      "         [ 0.5579,  0.8278,  1.4489, -0.2451]]])\n",
      "x.transpose(1, 2): \n",
      " tensor([[[-0.6576,  0.7237, -0.3415],\n",
      "         [-0.0910, -0.8033, -0.3925],\n",
      "         [ 0.6779,  0.9599, -0.8440],\n",
      "         [ 1.7254, -1.4178,  0.2096]],\n",
      "\n",
      "        [[-0.7420,  1.1749,  0.5579],\n",
      "         [-1.5567,  0.9946,  0.8278],\n",
      "         [-2.0906, -0.6373,  1.4489],\n",
      "         [-0.9844,  0.4512, -0.2451]]])\n",
      "Raw weights x * x^T: \n",
      " tensor([[[ 3.8774, -2.1985,  0.0499],\n",
      "         [-2.1985,  4.1004, -1.0393],\n",
      "         [ 0.0499, -1.0393,  1.0270]],\n",
      "\n",
      "        [[ 8.3136, -1.5319, -4.4904],\n",
      "         [-1.5319,  2.9795,  0.4448],\n",
      "         [-4.4904,  0.4448,  3.1559]]])\n",
      "Weights: \n",
      " tensor([[[9.7650e-01, 2.2437e-03, 2.1252e-02],\n",
      "         [1.8242e-03, 9.9236e-01, 5.8146e-03],\n",
      "         [2.5041e-01, 8.4267e-02, 6.6532e-01]],\n",
      "\n",
      "        [[9.9994e-01, 5.2981e-05, 2.7498e-06],\n",
      "         [1.0073e-02, 9.1721e-01, 7.2720e-02],\n",
      "         [4.4786e-04, 6.2294e-02, 9.3726e-01]]])\n",
      "y: \n",
      " tensor([[[-0.6478, -0.0990,  0.6461,  1.6862],\n",
      "         [ 0.7150, -0.7996,  0.9489, -1.4026],\n",
      "         [-0.3309, -0.3516, -0.3109,  0.4521]],\n",
      "\n",
      "        [[-0.7419, -1.5565, -2.0905, -0.9843],\n",
      "         [ 1.1108,  0.9568, -0.5002,  0.3861],\n",
      "         [ 0.5957,  0.8371,  1.3174, -0.2021]]])\n"
     ]
    }
   ],
   "source": [
    "x = torch.randn(2, 3, 4)\n",
    "print(\"x: \\n\", x)\n",
    "# Matrix multiplication over the last two dimensions\n",
    "print(\"x.transpose(1, 2): \\n\", x.transpose(1, 2))\n",
    "raw_weights = torch.bmm(x, x.transpose(1, 2))  # (b, t, k) * (b, k, t) -> (b, t, t)\n",
    "print(\"Raw weights x * x^T: \\n\", raw_weights)\n",
    "# Softmax over the last dimension (row)\n",
    "weights = F.softmax(raw_weights, dim=2)  # (b, t, t)\n",
    "print(\"Weights: \\n\", weights)\n",
    "# Weighted sum over the last dimension (row)\n",
    "y = torch.bmm(weights, x)  # (b, t, t) * (b, t, k) -> (b, t, k)\n",
    "print(\"y: \\n\", y)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-06T07:59:15.814281492Z",
     "start_time": "2023-10-06T07:59:15.540302895Z"
    }
   },
   "id": "df1e8403061df206"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Multi-head attention\n",
    ">Multi-head self-attention allows a model to jointly attend to information from different representation subspaces at different positions. With a single attention head, averaging inhibits this. ~[Attention is all you need](https://arxiv.org/abs/1706.03762)\n",
    " \n",
    "Based on [Peter Bloem's blog](https://peterbloem.nl/blog/transformers).\n",
    "- Multi-head self-attention allows the model to account for different relationships between different words in the input sequence.\n",
    "- A attention head is a self-attention layer with its own parameters. \n",
    "    - A self-attention layer consists of a linear projection of the input vectors, followed by the dot product attention and a linear projection of the output vectors.\n",
    "    - The projection matrices are denoted by $W_q^r, W_k^r, W_v^r$ for the $r$-th head.\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "b720dcaea8f0b14f"
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "outputs": [],
   "source": [
    "class SelfAttention(nn.Module):\n",
    "    def __init__(self, k_dimensions: int, heads: int = 4, mask: bool = False):\n",
    "        super().__init__()\n",
    "        assert k_dimensions % heads == 0, \"Embedding dimension must be divisible by number of heads.\"\n",
    "        self.k_dimensions = k_dimensions\n",
    "        self.heads = heads\n",
    "        \n",
    "        # Linear transformations for Q, K, V\n",
    "        self.transform_to_key = nn.Linear(k_dimensions, k_dimensions * heads, bias=False)\n",
    "        self.transform_to_query = nn.Linear(k_dimensions, k_dimensions * heads, bias=False)\n",
    "        self.transform_to_value = nn.Linear(k_dimensions, k_dimensions * heads, bias=False)\n",
    "        \n",
    "        # Final linear transformation\n",
    "        self.unify_layer = nn.Linear(k, k)\n",
    "        \n",
    "    def forward(self, x: torch.Tensor):\n",
    "        b_batch_size, t_sequence_length, k_dimensions = x.size()\n",
    "        h_heads = self.heads\n",
    "        \n",
    "        # Linear transformation to obtain key, query and value vectors\n",
    "        key_vectors = self.transform_to_key(x)  # (b, t, h * k)\n",
    "        query_vectors = self.transform_to_query(x)  # (b, t, h * k)\n",
    "        value_vectors = self.transform_to_value(x)  # (b, t, h * k)\n",
    "        \n",
    "        # We need one query, key and value vector per head, so we reshape in order to split the embedding dimension k into h heads\n",
    "        # A view is a reshape that doesn't change the underlying data representation\n",
    "        s = k_dimensions // h_heads\n",
    "        key_vectors = key_vectors.view(b_batch_size, t_sequence_length, h_heads, s)  # (b, t, h, s)\n",
    "        query_vectors = query_vectors.view(b_batch_size, t_sequence_length, h_heads, s)  # (b, t, h, s)\n",
    "        value_vectors = value_vectors.view(b_batch_size, t_sequence_length, h_heads, s)  # (b, t, h, s)\n",
    "        \n",
    "        # Dot product computation is the same for all heads, so we can fold the heads into the batch dimension\n",
    "        # (b, t, h, k) -> (b * h, t, k)\n",
    "        key_vectors = key_vectors.transpose(1, 2).contiguous().view(b_batch_size * h_heads, t_sequence_length, s)\n",
    "        query_vectors = query_vectors.transpose(1, 2).contiguous().view(b_batch_size * h_heads, t_sequence_length, s)\n",
    "        value_vectors = value_vectors.transpose(1, 2).contiguous().view(b_batch_size * h_heads, t_sequence_length, s)\n",
    "        \n",
    "        # Dot product of query and key vectors to obtain raw weights\n",
    "        dot_products = torch.bmm(query_vectors, key_vectors.transpose(1, 2))  # (b * h, t, t)\n",
    "        # Scale dot products by dimensionality of key\n",
    "        dot_products = dot_products / (k_dimensions**(1/2))\n",
    "        # Normalize weights\n",
    "        normalized = F.softmax(dot_products, dim=2)  # (b * h, t, t)\n",
    "        \n",
    "        # Apply the self-attention to the values\n",
    "        # (b * h, t, t) * (b * h, t, s) -> (b * h, t, s)\n",
    "        weighted_sum = torch.bmm(normalized, value_vectors).view(b_batch_size, h_heads, t_sequence_length, s)  # (b * h, t, s)\n",
    "        \n",
    "        # Put the heads back together by concatenating them\n",
    "        weighted_sum = weighted_sum.transpose(1, 2).contiguous().view(b_batch_size, t_sequence_length, k_dimensions)\n",
    "        return self.unify_layer(weighted_sum)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-06T08:59:19.205682601Z",
     "start_time": "2023-10-06T08:59:19.153888950Z"
    }
   },
   "id": "da3c7bc12915ebd2"
  },
  {
   "cell_type": "markdown",
   "source": [
    "![Reshape of tensors to iterate over heads](https://peterbloem.nl/files/transformers/reshape.svg)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "9312b1e93bdbc486"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Transformer block\n",
    "> The Transformer block is the basic building block of the Transformer architecture. It consists of a multi-head self-attention layer, followed by a feed-forward layer. Each of these layers has a residual connection around it, and is followed by a layer normalization. ~[Attention is all you need](https://arxiv.org/abs/1706.03762)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "7f2bee1d0111e152"
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "outputs": [],
   "source": [
    "class TransformerBlock(nn.Module):\n",
    "    attention: SelfAttention\n",
    "    mask: bool\n",
    "    \n",
    "    def __init__(self, k_dimensions: int, heads: int, mask: bool = False):\n",
    "        super().__init__()\n",
    "        self.attention = SelfAttention(k_dimensions, heads=heads, mask=mask)\n",
    "        self.mask = mask\n",
    "        \n",
    "        # Normalizatiion layers to normalize the input before the residual connection is added, and to normalize the output before it is passed on to the next layer.\n",
    "        self.norm1 = nn.LayerNorm(k_dimensions)\n",
    "        self.norm2 = nn.LayerNorm(k_dimensions)\n",
    "        \n",
    "        # Choice & form of feed-forward layer is arbitrary\n",
    "        self.feed_forward = nn.Sequential(\n",
    "            nn.Linear(k_dimensions, 4 * k_dimensions),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(4 * k_dimensions, k_dimensions)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x: torch.Tensor):\n",
    "        attended = self.attention(x)\n",
    "        x = self.norm1(attended + x)\n",
    "        fed_forward = self.feed_forward(x)\n",
    "        return self.norm2(fed_forward + x)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-06T08:59:20.815022284Z",
     "start_time": "2023-10-06T08:59:20.800423207Z"
    }
   },
   "id": "2fd99f6e93f66049"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Text classification transformer\n",
    "From [Peter Bloem's blog](https://peterbloem.nl/blog/transformers)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "75c18b746851d58b"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "class Transformer(nn.Module):\n",
    "    def __init__(self, k, heads, depth, seq_length, num_tokens, num_classes):\n",
    "        super().__init__()\n",
    "\n",
    "        self.num_tokens = num_tokens\n",
    "        self.token_emb = nn.Embedding(num_tokens, k)\n",
    "        self.pos_emb = nn.Embedding(seq_length, k)\n",
    "\n",
    "        # The sequence of transformer blocks that does all the\n",
    "        # heavy lifting\n",
    "        tblocks = []\n",
    "        for i in range(depth):\n",
    "            tblocks.append(TransformerBlock(k=k, heads=heads))\n",
    "        self.tblocks = nn.Sequential(*tblocks)\n",
    "\n",
    "        # Maps the final output sequence to class logits\n",
    "        self.toprobs = nn.Linear(k, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        :param x: A (b, t) tensor of integer values representing\n",
    "                  words (in some predetermined vocabulary).\n",
    "        :return: A (b, c) tensor of log-probabilities over the\n",
    "                 classes (where c is the nr. of classes).\n",
    "        \"\"\"\n",
    "        # generate token embeddings\n",
    "        tokens = self.token_emb(x)\n",
    "        b, t, k = tokens.size()\n",
    "\n",
    "        # generate position embeddings\n",
    "        positions = torch.arange(t)\n",
    "    positions = self.pos_emb(positions)[None, :, :].expand(b, t, k)\n",
    "\n",
    "    x = tokens + positions\n",
    "    x = self.tblocks(x)\n",
    "\n",
    "    # Average-pool over the t dimension and project to class\n",
    "    # probabilities\n",
    "    x = self.toprobs(x.mean(dim=1))\n",
    "    return F.log_softmax(x, dim=1)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "51026023aa6c22ab"
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-06T07:59:15.814917145Z",
     "start_time": "2023-10-06T07:59:15.589510864Z"
    }
   },
   "id": "6dfafda0d5ee81d4"
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-06T07:59:15.815505010Z",
     "start_time": "2023-10-06T07:59:15.589760930Z"
    }
   },
   "id": "28ddf09c77b52d19"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
